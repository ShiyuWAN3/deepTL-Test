n.hidden = c(20, 15, 10), n.epoch = n.epoch.each, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = FALSE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam")
f0.x <- predict(f0.x.mod, cbind(x, z))
h_obj <- importDnnet(y = factor(ifelse(h.sim == 1, "A", "B")), x = x)
split.h0 <- splitDnnet(h_obj, boots.sample)
g.x.mod <- dnnet(split.h0$train, split.h0$valid, load.param = ifelse(i == 1, FALSE, TRUE),
initial.param = g.x.mod,
n.hidden = c(20, 15, 10), n.epoch = n.epoch.each, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = FALSE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam")
g.x <- predict(g.x.mod, x)[, "A"]
# f1.x <- predict(lm(y ~ x + z + x:z, subset = (h.sim == 1)), data.frame(x, z))
# f0.x <- predict(lm(y ~ x + z + x:z, subset = (h.sim == 0)), data.frame(x, z))
# g.x <- fitted(glm(h.sim ~ x, family = "binomial"))
sigma.sq <- (sum((y[h.sim == 1] - f1.x[h.sim == 1])**2) +
sum((y[h.sim == 0] - f0.x[h.sim == 0])**2))/length(y)
h.sim.d <- -((y - f1.x)**2 - (y - f0.x)**2)/sigma.sq/2 + log(g.x/(1-g.x))
h.sim <- rbinom(n_sample, 1, 1/(1+exp(-h.sim.d))) # (z.sim.d > 0)*1 #
loss.f1 <- c(loss.f1, f1.x.mod@loss)
loss.f0 <- c(loss.f0, f0.x.mod@loss)
loss.gx <- c(loss.gx, g.x.mod@loss)
print(c(mean(h.sim == h),
mean((g.x > 0.5) == h),
min(-mean(prob.h*log(g.x) + (1-prob.h)*log(1-g.x)),
-mean((1-prob.h)*log(g.x) + prob.h*log(1-g.x)))))
}
plot(loss.f1)
plot(loss.f2)
plot(loss.gx)
h.sim <- rbinom(n_sample, 1, 0.5)
loss.f1 <- loss.f0 <- loss.gx <- c()
f1.x.mod <- f0.x.mod <- g.x.mod <- NULL
n.epoch.each <- 100
boots.sample <- sample(n_sample, replace = TRUE)
for(i in 1:n.iter) {
y_obj <- importDnnet(y = y, x = cbind(x, z))
y_obj_split <- splitDnnet(y_obj, boots.sample)
y_obj_train <- y_obj_split$train
y_obj_valid <- y_obj_split$valid
split.y1 <- splitDnnet(y_obj_train, which(h.sim[boots.sample] == 1))
f1.x.mod <- dnnet(split.y1$train, y_obj_valid, load.param = ifelse(i == 1, FALSE, TRUE),
initial.param = f1.x.mod,
n.hidden = c(20, 15, 10), n.epoch = n.epoch.each, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = FALSE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam")
f1.x <- predict(f1.x.mod, cbind(x, z))
split.y0 <- splitDnnet(y_obj_train, which(h.sim[boots.sample] == 0))
f0.x.mod <- dnnet(split.y0$train, y_obj_valid, load.param = ifelse(i == 1, FALSE, TRUE),
initial.param = f0.x.mod,
n.hidden = c(20, 15, 10), n.epoch = n.epoch.each, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = FALSE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam")
f0.x <- predict(f0.x.mod, cbind(x, z))
h_obj <- importDnnet(y = factor(ifelse(h.sim == 1, "A", "B")), x = x)
split.h0 <- splitDnnet(h_obj, boots.sample)
g.x.mod <- dnnet(split.h0$train, split.h0$valid, load.param = ifelse(i == 1, FALSE, TRUE),
initial.param = g.x.mod,
n.hidden = c(20, 15, 10), n.epoch = n.epoch.each, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = FALSE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam")
g.x <- predict(g.x.mod, x)[, "A"]
# f1.x <- predict(lm(y ~ x + z + x:z, subset = (h.sim == 1)), data.frame(x, z))
# f0.x <- predict(lm(y ~ x + z + x:z, subset = (h.sim == 0)), data.frame(x, z))
# g.x <- fitted(glm(h.sim ~ x, family = "binomial"))
sigma.sq <- (sum((y[h.sim == 1] - f1.x[h.sim == 1])**2) +
sum((y[h.sim == 0] - f0.x[h.sim == 0])**2))/length(y)
h.sim.d <- -((y - f1.x)**2 - (y - f0.x)**2)/sigma.sq/2 + log(g.x/(1-g.x))
h.sim <- rbinom(n_sample, 1, 1/(1+exp(-h.sim.d))) # (z.sim.d > 0)*1 #
loss.f1 <- c(loss.f1, f1.x.mod@loss)
loss.f0 <- c(loss.f0, f0.x.mod@loss)
loss.gx <- c(loss.gx, g.x.mod@loss)
print(c(mean(h.sim == h),
mean((g.x > 0.5) == h),
min(-mean(prob.h*log(g.x) + (1-prob.h)*log(1-g.x)),
-mean((1-prob.h)*log(g.x) + prob.h*log(1-g.x)))))
}
plot(loss.f1)
plot(loss.f2)
plot(loss.gx)
plot(loss.f0)
library(deepTL)
library(MASS)
n_sample <- 1000
n_test <- 1000
n_p <- 5
n.iter <- 100
set.seed(1000)
beta0 <- rnorm(n_p)
beta1 <- rnorm(n_p)
beta2 <- rnorm(n_p)
beta3 <- rnorm(n_p)
beta4 <- rnorm(n_p)
beta5 <- rnorm(n_p)
cbind(beta0, beta1, beta2, beta3, beta4, beta5)
set.seed(99999)
n.rep <- 1000
mse.f1.lm <- mse.f0.lm <- mse.yg.lm <- div.gx.lm <- acc.gx.lm <- numeric(n.rep)
mse.f1.or <- mse.f0.or <- mse.yg.or <- div.gx.or <- acc.gx.or <- numeric(n.rep)
for (j in 1:n.rep) {
x <- mvrnorm(n_sample, rep(0, n_p), diag(n_p))
prob.h <- as.numeric(1/(1+exp(x %*% beta0 * 2 + x[, 1]**2 - x[, 2]**2)))
h <- rbinom(n_sample, 1, prob.h)
prob.z <- as.numeric(1/(1+exp(x %*% beta1))) # 0.5 #
z <- rbinom(n_sample, 1, prob.z)
y <- ifelse(h,
x %*% beta2 + x[, 1]*x[, 2]*2 + z * (x %*% beta3 + x[, 1]**2 + x[, 2]**2 + 1),
x %*% beta2 + x[, 1]*x[, 2]*2 + z * (x %*% beta3 - x[, 1]**2 - x[, 2]**2 - 1)) * 2 + rnorm(n_sample)
x.test <- mvrnorm(n_test, rep(0, n_p), diag(n_p))
prob.h.test <- as.numeric(1/(1+exp(x.test %*% beta0 * 2 + x.test[, 1]**2 - x.test[, 2]**2)))
h.test <- rbinom(n_test, 1, prob.h.test)
prob.z.test <- as.numeric(1/(1+exp(x.test %*% beta1))) # 0.5 #
z.test <- rbinom(n_test, 1, prob.z.test)
y.test <- ifelse(h.test,
x.test %*% beta2 + x.test[, 1]*x.test[, 2]*2 +
z.test * (x.test %*% beta3 + x.test[, 1]**2 + x.test[, 2]**2 + 1),
x.test %*% beta2 + x.test[, 1]*x.test[, 2]*2 +
z.test * (x.test %*% beta3 - x.test[, 1]**2 - x.test[, 2]**2 - 1)) * 2 + rnorm(n_test)
h.sim <- rbinom(n_sample, 1, 0.5)
for(i in 1:n.iter) {
y1_obj <- importDnnet(y = y[h.sim == 1], x = cbind(x[h.sim == 1, ], z[h.sim == 1]))
f1.x.mod <- ensemble_dnnet(y1_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 500, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
f1.x <- predict(f1.x.mod, cbind(x, z))
y0_obj <- importDnnet(y = y[h.sim == 0], x = cbind(x[h.sim == 0, ], z[h.sim == 0]))
f0.x.mod <- ensemble_dnnet(y0_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 500, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
f0.x <- predict(f0.x.mod, cbind(x, z))
h_obj <- importDnnet(y = factor(ifelse(h.sim == 1, "A", "B")), x = x)
g.x.mod <- ensemble_dnnet(h_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 250, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
g.x <- predict(g.x.mod, x)[, "A"]
# f1.x <- predict(lm(y ~ x + z + x:z, subset = (h.sim == 1)), data.frame(x, z))
# f0.x <- predict(lm(y ~ x + z + x:z, subset = (h.sim == 0)), data.frame(x, z))
# g.x <- fitted(glm(h.sim ~ x, family = "binomial"))
sigma.sq <- (sum((y[h.sim == 1] - f1.x[h.sim == 1])**2) +
sum((y[h.sim == 0] - f0.x[h.sim == 0])**2))/length(y)
h.sim.d <- -((y - f1.x)**2 - (y - f0.x)**2)/sigma.sq/2 + log(g.x/(1-g.x))
h.sim <- rbinom(n_sample, 1, 1/(1+exp(-h.sim.d))) # (z.sim.d > 0)*1 #
print(c(mean(h.sim == h),
mean((g.x > 0.5) == h),
min(-mean(prob.h*log(g.x) + (1-prob.h)*log(1-g.x)),
-mean((1-prob.h)*log(g.x) + prob.h*log(1-g.x)))))
}
# plot(g.x ~ prob.h)
# table(h.sim, h)
# table(g.x > 0.5, h)
g.x.test <- predict(g.x.mod, x.test)[, "A"]
f1x.test <- predict(f1.x.mod, cbind(x.test, z.test))
f0x.test <- predict(f0.x.mod, cbind(x.test, z.test))
ypd.test <- ifelse(g.x.test > 0.5, f1x.test, f0x.test)
# plot(y.test, ypd.test, col = h.test + 1, bg = (g.x.test > 0.5) + 1, pch = 21)
# abline(0, 1)
# cor(y.test, ypd.test)
# table(g.x.test > 0.5, h.test)
mse.f1.lm[j] <- min(mean((y.test[h.test == 1] - f1x.test[h.test == 1])**2),
mean((y.test[h.test == 1] - f0x.test[h.test == 1])**2))
mse.f0.lm[j] <- min(mean((y.test[h.test == 0] - f1x.test[h.test == 0])**2),
mean((y.test[h.test == 0] - f0x.test[h.test == 0])**2))
mse.yg.lm[j] <- mean((y.test - ypd.test)**2)
div.gx.lm[j] <- min(-mean(prob.h.test*log(g.x.test) + (1 - prob.h.test)*log(1 - g.x.test)),
-mean(prob.h.test*log(1 - g.x.test) + (1 - prob.h.test)*log(g.x.test)))
acc.gx.lm[j] <- max(mean((g.x.test > 0.5) == h.test), mean((g.x.test <= 0.5) == h.test))
oracle.lm1 <- as.numeric(cbind(rep(1, n_test), x.test, z.test, x.test*z.test) %*%
coefficients(lm(y ~ x + z + x:z, subset = (h == 1))))
oracle.lm0 <- as.numeric(cbind(rep(1, n_test), x.test, z.test, x.test*z.test) %*%
coefficients(lm(y ~ x + z + x:z, subset = (h == 0))))
oracle.glm <- as.numeric(1/(1+exp(-cbind(rep(1, n_test), x.test) %*%
coefficients(glm(h ~ x, family = "binomial")))))
oracle.ypd <- ifelse((oracle.glm > 0.5), oracle.lm1, oracle.lm0)
mse.f1.or[j] <- mean((y.test - oracle.lm1)[h.test == 1]**2)
mse.f0.or[j] <- mean((y.test - oracle.lm0)[h.test == 0]**2)
mse.yg.or[j] <- mean((y.test - oracle.ypd)**2)
div.gx.or[j] <- -mean(prob.h.test*log(oracle.glm) + (1 - prob.h.test)*log(1 - oracle.glm))
acc.gx.or[j] <- max(mean((oracle.glm > 0.5) == h.test), mean((oracle.glm <= 0.5) == h.test))
if(! j %% 100) print(j)
}
par(mfrow = 2:3)
library(deepTL)
library(MASS)
n_sample <- 1000
n_test <- 1000
n_p <- 5
n.iter <- 100
set.seed(1000)
beta0 <- rnorm(n_p)
beta1 <- rnorm(n_p)
beta2 <- rnorm(n_p)
beta3 <- rnorm(n_p)
beta4 <- rnorm(n_p)
beta5 <- rnorm(n_p)
cbind(beta0, beta1, beta2, beta3, beta4, beta5)
set.seed(99999)
n.rep <- 1000
mse.f1.lm <- mse.f0.lm <- mse.yg.lm <- div.gx.lm <- acc.gx.lm <- numeric(n.rep)
mse.f1.or <- mse.f0.or <- mse.yg.or <- div.gx.or <- acc.gx.or <- numeric(n.rep)
for (j in 1:n.rep) {
x <- mvrnorm(n_sample, rep(0, n_p), diag(n_p))
prob.h <- as.numeric(1/(1+exp(x %*% beta0 * 2 + x[, 1]**2 - x[, 2]**2)))
h <- rbinom(n_sample, 1, prob.h)
prob.z <- as.numeric(1/(1+exp(x %*% beta1))) # 0.5 #
z <- rbinom(n_sample, 1, prob.z)
y <- ifelse(h,
x %*% beta2 + x[, 1]*x[, 2]*2 + z * (x %*% beta3 + x[, 1]**2 + x[, 2]**2 + 1),
x %*% beta2 + x[, 1]*x[, 2]*2 + z * (x %*% beta3 - x[, 1]**2 - x[, 2]**2 - 1)) * 2 + rnorm(n_sample)
x.test <- mvrnorm(n_test, rep(0, n_p), diag(n_p))
prob.h.test <- as.numeric(1/(1+exp(x.test %*% beta0 * 2 + x.test[, 1]**2 - x.test[, 2]**2)))
h.test <- rbinom(n_test, 1, prob.h.test)
prob.z.test <- as.numeric(1/(1+exp(x.test %*% beta1))) # 0.5 #
z.test <- rbinom(n_test, 1, prob.z.test)
y.test <- ifelse(h.test,
x.test %*% beta2 + x.test[, 1]*x.test[, 2]*2 +
z.test * (x.test %*% beta3 + x.test[, 1]**2 + x.test[, 2]**2 + 1),
x.test %*% beta2 + x.test[, 1]*x.test[, 2]*2 +
z.test * (x.test %*% beta3 - x.test[, 1]**2 - x.test[, 2]**2 - 1)) * 2 + rnorm(n_test)
h.sim <- rbinom(n_sample, 1, 0.5)
for(i in 1:n.iter) {
y1_obj <- importDnnet(y = y[h.sim == 1], x = cbind(x[h.sim == 1, ], z[h.sim == 1]))
f1.x.mod <- ensemble_dnnet(y1_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 500, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
f1.x <- predict(f1.x.mod, cbind(x, z))
y0_obj <- importDnnet(y = y[h.sim == 0], x = cbind(x[h.sim == 0, ], z[h.sim == 0]))
f0.x.mod <- ensemble_dnnet(y0_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 500, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
f0.x <- predict(f0.x.mod, cbind(x, z))
h_obj <- importDnnet(y = factor(ifelse(h.sim == 1, "A", "B")), x = x)
g.x.mod <- ensemble_dnnet(h_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 250, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
g.x <- predict(g.x.mod, x)[, "A"]
# f1.x <- predict(lm(y ~ x + z + x:z, subset = (h.sim == 1)), data.frame(x, z))
# f0.x <- predict(lm(y ~ x + z + x:z, subset = (h.sim == 0)), data.frame(x, z))
# g.x <- fitted(glm(h.sim ~ x, family = "binomial"))
sigma.sq <- (sum((y[h.sim == 1] - f1.x[h.sim == 1])**2) +
sum((y[h.sim == 0] - f0.x[h.sim == 0])**2))/length(y)
h.sim.d <- -((y - f1.x)**2 - (y - f0.x)**2)/sigma.sq/2 + log(g.x/(1-g.x))
h.sim <- rbinom(n_sample, 1, 1/(1+exp(-h.sim.d))) # (z.sim.d > 0)*1 #
print(c(mean(h.sim == h),
mean((g.x > 0.5) == h),
min(-mean(prob.h*log(g.x) + (1-prob.h)*log(1-g.x)),
-mean((1-prob.h)*log(g.x) + prob.h*log(1-g.x)))))
}
# plot(g.x ~ prob.h)
# table(h.sim, h)
# table(g.x > 0.5, h)
g.x.test <- predict(g.x.mod, x.test)[, "A"]
f1x.test <- predict(f1.x.mod, cbind(x.test, z.test))
f0x.test <- predict(f0.x.mod, cbind(x.test, z.test))
ypd.test <- ifelse(g.x.test > 0.5, f1x.test, f0x.test)
# plot(y.test, ypd.test, col = h.test + 1, bg = (g.x.test > 0.5) + 1, pch = 21)
# abline(0, 1)
# cor(y.test, ypd.test)
# table(g.x.test > 0.5, h.test)
mse.f1.lm[j] <- min(mean((y.test[h.test == 1] - f1x.test[h.test == 1])**2),
mean((y.test[h.test == 1] - f0x.test[h.test == 1])**2))
mse.f0.lm[j] <- min(mean((y.test[h.test == 0] - f1x.test[h.test == 0])**2),
mean((y.test[h.test == 0] - f0x.test[h.test == 0])**2))
mse.yg.lm[j] <- mean((y.test - ypd.test)**2)
div.gx.lm[j] <- min(-mean(prob.h.test*log(g.x.test) + (1 - prob.h.test)*log(1 - g.x.test)),
-mean(prob.h.test*log(1 - g.x.test) + (1 - prob.h.test)*log(g.x.test)))
acc.gx.lm[j] <- max(mean((g.x.test > 0.5) == h.test), mean((g.x.test <= 0.5) == h.test))
oracle.lm1 <- as.numeric(cbind(rep(1, n_test), x.test, z.test, x.test*z.test) %*%
coefficients(lm(y ~ x + z + x:z, subset = (h == 1))))
oracle.lm0 <- as.numeric(cbind(rep(1, n_test), x.test, z.test, x.test*z.test) %*%
coefficients(lm(y ~ x + z + x:z, subset = (h == 0))))
oracle.glm <- as.numeric(1/(1+exp(-cbind(rep(1, n_test), x.test) %*%
coefficients(glm(h ~ x, family = "binomial")))))
oracle.ypd <- ifelse((oracle.glm > 0.5), oracle.lm1, oracle.lm0)
mse.f1.or[j] <- mean((y.test - oracle.lm1)[h.test == 1]**2)
mse.f0.or[j] <- mean((y.test - oracle.lm0)[h.test == 0]**2)
mse.yg.or[j] <- mean((y.test - oracle.ypd)**2)
div.gx.or[j] <- -mean(prob.h.test*log(oracle.glm) + (1 - prob.h.test)*log(1 - oracle.glm))
acc.gx.or[j] <- max(mean((oracle.glm > 0.5) == h.test), mean((oracle.glm <= 0.5) == h.test))
if(! j %% 100) print(j)
}
n_sample <- 1000
n_test <- 1000
n_p <- 5
n.iter <- 100
set.seed(1000)
beta0 <- rnorm(n_p)
beta1 <- rnorm(n_p)
beta2 <- rnorm(n_p)
beta3 <- rnorm(n_p)
beta4 <- rnorm(n_p)
beta5 <- rnorm(n_p)
x <- mvrnorm(n_sample, rep(0, n_p), diag(n_p))
prob.h <- as.numeric(1/(1+exp(x %*% beta0 * 2 + x[, 1]**2 - x[, 2]**2)))
h <- rbinom(n_sample, 1, prob.h)
prob.z <- as.numeric(1/(1+exp(x %*% beta1))) # 0.5 #
z <- rbinom(n_sample, 1, prob.z)
y <- ifelse(h,
x %*% beta2 + x[, 1]*x[, 2]*2 + z * (x %*% beta3 + x[, 1]**2 + x[, 2]**2 + 1),
x %*% beta2 + x[, 1]*x[, 2]*2 + z * (x %*% beta3 - x[, 1]**2 - x[, 2]**2 - 1)) * 2 + rnorm(n_sample)
x.test <- mvrnorm(n_test, rep(0, n_p), diag(n_p))
prob.h.test <- as.numeric(1/(1+exp(x.test %*% beta0 * 2 + x.test[, 1]**2 - x.test[, 2]**2)))
h.test <- rbinom(n_test, 1, prob.h.test)
prob.z.test <- as.numeric(1/(1+exp(x.test %*% beta1))) # 0.5 #
z.test <- rbinom(n_test, 1, prob.z.test)
y.test <- ifelse(h.test,
x.test %*% beta2 + x.test[, 1]*x.test[, 2]*2 +
z.test * (x.test %*% beta3 + x.test[, 1]**2 + x.test[, 2]**2 + 1),
x.test %*% beta2 + x.test[, 1]*x.test[, 2]*2 +
z.test * (x.test %*% beta3 - x.test[, 1]**2 - x.test[, 2]**2 - 1)) * 2 + rnorm(n_test)
y_obj <- importDnnet(x = cbind(x, z), y = y)
f.base.mod <- ensemble_dnnet(y_obj, 100,
esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 500, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
f.xz <- predict(f.base.mod, cbind(x, z))
h.sim <- rbinom(n_sample, 1, 0.5)
for(i in 1:n.iter) {
h_obj <- importDnnet(x = x, y = factor(ifelse(h.sim == 1, "A", "B")))
g.x.mod <- ensemble_dnnet(h_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 250, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
g.x <- predict(g.x.mod, x)[, "A"]
y0_obj <- importDnnet(x = cbind(x, z), y = (y - f.xz)/(h.sim - g.x), w = (h.sim - g.x)**2)
h.xz.mod <- ensemble_dnnet(y0_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 1000, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
h.xz <- predict(h.xz.mod, cbind(x, z))
sigma.sq <- mean((y - f.xz - (h.sim - g.x)*h.xz)**2)
h.sim.d <- -((y - f.xz - (1 - g.x)*h.xz)**2 - (y - f.xz + g.x*h.xz)**2)/sigma.sq/2 + log(g.x/(1-g.x))
h.sim <- rbinom(n_sample, 1, 1/(1+exp(-h.sim.d))) # (z.sim.d > 0)*1 #
print(c(i,
mean(h.sim == h),
mean((g.x > 0.5) == h),
min(-mean(prob.h*log(g.x) + (1-prob.h)*log(1-g.x)),
-mean((1-prob.h)*log(g.x) + prob.h*log(1-g.x))),
min(-mean(h*log(g.x) + (1-h)*log(1-g.x)),
-mean((1-h)*log(g.x) + h*log(1-g.x)))))
}
par(mfrow = 2:3)
h.sim <- rbinom(n_sample, 1, 0.5)
for(i in 1:n.iter) {
h_obj <- importDnnet(x = x, y = factor(ifelse(h.sim == 1, "A", "B")))
g.x.mod <- ensemble_dnnet(h_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 250, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
g.x <- predict(g.x.mod, x)[, "A"]
y0_obj <- importDnnet(x = cbind(x, z), y = (y - f.xz)/(h.sim - g.x), w = (h.sim - g.x)**2)
h.xz.mod <- ensemble_dnnet(y0_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 1000, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
h.xz <- predict(h.xz.mod, cbind(x, z))
sigma.sq <- mean((y - f.xz - (h.sim - g.x)*h.xz)**2)
h.sim.d <- -((y - f.xz - (1 - g.x)*h.xz)**2 - (y - f.xz + g.x*h.xz)**2)/sigma.sq/2 + log(g.x/(1-g.x))
h.sim <- rbinom(n_sample, 1, 1/(1+exp(-h.sim.d))) # (z.sim.d > 0)*1 #
print(c(i,
mean(h.sim == h),
mean((g.x > 0.5) == h),
min(-mean(prob.h*log(g.x) + (1-prob.h)*log(1-g.x)),
-mean((1-prob.h)*log(g.x) + prob.h*log(1-g.x))),
min(-mean(h*log(g.x) + (1-h)*log(1-g.x)),
-mean((1-h)*log(g.x) + h*log(1-g.x)))))
}
h.sim <- ifelse(y - f.xz > median(y - f.xz), 1, 0)
# h.sim <- rbinom(n_sample, 1, 0.5)
for(i in 1:n.iter) {
h_obj <- importDnnet(x = x, y = factor(ifelse(h.sim == 1, "A", "B")))
g.x.mod <- ensemble_dnnet(h_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 250, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
g.x <- predict(g.x.mod, x)[, "A"]
y0_obj <- importDnnet(x = cbind(x, z), y = (y - f.xz)/(h.sim - g.x), w = (h.sim - g.x)**2)
h.xz.mod <- ensemble_dnnet(y0_obj, 25, esCtrl = list(n.hidden = c(20, 15, 10), n.epoch = 1000, n.batch = 50,
early.stop = TRUE, early.stop.det = 1000, plot = TRUE,
norm.x = FALSE, norm.y = TRUE,
learning.rate.adaptive = "adam"))
h.xz <- predict(h.xz.mod, cbind(x, z))
sigma.sq <- mean((y - f.xz - (h.sim - g.x)*h.xz)**2)
h.sim.d <- -((y - f.xz - (1 - g.x)*h.xz)**2 - (y - f.xz + g.x*h.xz)**2)/sigma.sq/2 + log(g.x/(1-g.x))
h.sim <- rbinom(n_sample, 1, 1/(1+exp(-h.sim.d))) # (z.sim.d > 0)*1 #
print(c(i,
mean(h.sim == h),
mean((g.x > 0.5) == h),
min(-mean(prob.h*log(g.x) + (1-prob.h)*log(1-g.x)),
-mean((1-prob.h)*log(g.x) + prob.h*log(1-g.x))),
min(-mean(h*log(g.x) + (1-h)*log(1-g.x)),
-mean((1-h)*log(g.x) + h*log(1-g.x)))))
}
rm(list = ls())
library(deepTL)
library(MASS)
library(randomForest)
#### DML method ####
dml_method <- function(x, y, z, method = "rf", ntree = 2000) {
n <- dim(x)[1]
###### Divide the data for cross fitting
ind1 <- sample(1:n, floor(n/2))
x1 <- x[ind1, ]
y1 <- y[ind1]
z1 <- z[ind1]
x2 <- x[-ind1, ]
y2 <- y[-ind1]
z2 <- z[-ind1]
#### DML - PLM
pred1 <- dml(x2, y2, z2, x1, y1, z1, method, ntree)
pred2 <- dml(x1, y1, z1, x2, y2, z2, method, ntree)
theta1 <- sum((y1 - pred1[, 1])*(z1 - pred1[, 2]))/sum((z1 - pred1[, 2])**2)
theta2 <- sum((y2 - pred2[, 1])*(z2 - pred2[, 2]))/sum((z2 - pred2[, 2])**2)
theta <- (theta1 + theta2) / 2
pred <- rbind(pred1, pred2)
y_ <- c(y1, y2)
z_ <- c(z1, z2)
var_est <- mean((z_ - pred[, 2])**2*(y_ - pred[, 1] - theta*(z_ - pred[, 2]))**2) /
mean((z_ - pred[, 2])**2)**2 / n
#### DML - DR
theta1_dr <- mean((z1*y1 - pred1[, 3]*(z1 - pred1[, 2]))/pred1[, 2] -
((1 - z1)*y1 + pred1[, 4]*(z1 - pred1[, 2]))/(1 - pred1[, 2]))
theta2_dr <- mean((z2*y2 - pred2[, 3]*(z2 - pred2[, 2]))/pred2[, 2] -
((1 - z2)*y2 + pred2[, 4]*(z2 - pred2[, 2]))/(1 - pred2[, 2]))
theta_dr <- (theta1_dr + theta2_dr) / 2
theta_dr_var <- mean(((z_*y_ - pred[, 3]*(z_ - pred[, 2]))/pred[, 2] -
((1 - z_)*y_ + pred[, 4]*(z_ - pred[, 2]))/(1 - pred[, 2]))**2) / n
return(c(theta, var_est, theta_dr, theta_dr_var))
}
#### called by "dml_method"
dml <- function(x_train, y_train, z_train,
x_valid, y_valid, z_valid,
method, ntree) {
mod_y <- randomForest(x_train, y_train, ntree = ntree)
mod_z <- randomForest(x_train, factor(z_train), ntree = ntree)
mod_y1 <- randomForest(x_train[z_train == 1, ], y_train[z_train == 1], ntree = ntree)
mod_y0 <- randomForest(x_train[z_train == 0, ], y_train[z_train == 0], ntree = ntree)
ey <- predict(mod_y, x_valid)
ez <- predict(mod_z, x_valid, type = "prob")[, "1"]
ey1 <- predict(mod_y1, x_valid)
ey0 <- predict(mod_y0, x_valid)
return(cbind(ey, ez, ey1, ey0))
}
#### Simulation Scenarios ####
p.use <- 5
set.seed(10086)
seed <- floor(runif(1)*100000)
alpha1 <- runif(p.use) * 2 - 1
alpha2 <- runif(p.use) * 2 - 1
prob_z_sce <- function(scenario, x, alpha1 = alpha1) {
if(scenario == 1)
return(as.numeric(1/(1 + exp(1 - x[, 1:p.use] %*% alpha1))))
if(scenario == 2)
return(as.numeric(1/(1 + exp(1 - x[, 1:p.use] %*% alpha1 - x[, 8]**2))))
if(scenario == 3)
return(as.numeric(1/(1 + exp(1 - x[, 1:p.use] %*% alpha1 - 2 * cos(x[, 8])))))
}
y_sce <- function(scenario, x, beta.true, z, sigma, alpha2 = alpha2) {
if(scenario == 1)
return(-1 + beta.true * z + as.numeric(x[, 1:p.use] %*% alpha2) + rnorm(length(z)) * sigma)
if(scenario == 2)
return(-1 + beta.true * z + as.numeric(x[, 1:p.use] %*% alpha2) + abs(x[, 8]) * x[, 9]**2 + rnorm(length(z)) * sigma)
if(scenario == 3)
return(-1 + beta.true * z + as.numeric(x[, 1:p.use] %*% alpha2) - cos(x[, 8] * 2) + x[, 9]**2 + rnorm(length(z)) * sigma)
}
lm_naive <- function(x, y, z) {
lm_mod <- lm(y ~ z + x)
return(data.frame(method = "lm-naive",
beta = coefficients(lm_mod)["z"],
var = vcov(lm_mod)["z", "z"]))
}
ps_naive <- function(x, y, z) {
ps_glm <- fitted(glm(z ~ x, family = "binomial"))
lm_mod <- lm(y ~ z + ps_glm)
return(data.frame(method = "ps-naive",
beta = coefficients(lm_mod)["z"],
var = vcov(lm_mod)["z", "z"]))
}
lm_oracle <- function(x, y, z, scenario) {
if(scenario == 2)
x <- cbind(x, abs(x[, 8]) * x[, 9]**2)
if(scenario == 3)
x <- cbind(x, cos(x[, 8] * 2), x[, 9]**2)
lm_mod <- lm(y ~ z + x)
return(data.frame(method = "lm-oracle",
beta = coefficients(lm_mod)["z"],
var = vcov(lm_mod)["z", "z"]))
}
library(devtools)
install.packages(c("devtools", "digest", "pkgbuild", "pkgload", "ps", "rlang", "stringi"))
library(devtools)
install_github("SkadiEye/deepTL")
install.packages("pkgbuild")
library(devtools)
install_github("SkadiEye/deepTL")
library(devtools)
install_github("SkadiEye/deepTL")
install_github("SkadiEye/ITRlearn")
install_github("SkadiEye/dnnet")
install_github("SkadiEye/ITRlearn")
install_github("SkadiEye/ITRlearn", type = "source")
