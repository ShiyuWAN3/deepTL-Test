% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/2-2-dnnet.R
\name{dnnet.backprop.r}
\alias{dnnet.backprop.r}
\title{Back Propagation}
\usage{
dnnet.backprop.r(n.hidden, w.ini, load.param, initial.param, x, y, w,
  valid, x.valid, y.valid, w.valid, activate, activate_, n.epoch, n.batch,
  model.type, learning.rate, l1.reg, l2.reg, early.stop, early.stop.det,
  learning.rate.adaptive, rho, epsilon, beta1, beta2, loss.f)
}
\arguments{
\item{n.hidden}{A numeric vector for numbers of nodes for all hidden layers.}

\item{w.ini}{Initial weight parameter.}

\item{load.param}{Whether initial parameters are loaded into the model.}

\item{initial.param}{The initial parameters to be loaded.}

\item{x}{x}

\item{y}{y}

\item{w}{w}

\item{valid}{If exists the validation set}

\item{x.valid}{x-valid}

\item{y.valid}{y-valid}

\item{w.valid}{w-valid}

\item{activate}{Activation Function.}

\item{activate_}{The forst derivative of the activation function.}

\item{n.epoch}{Maximum number of epochs.}

\item{n.batch}{Batch size for batch gradient descent.}

\item{model.type}{Type of model.}

\item{learning.rate}{Initial learning rate, 0.001 by default; If "adam" is chosen as
an adaptive learning rate adjustment method, 0.1 by defalut.}

\item{l1.reg}{weight for l1 regularization, optional.}

\item{l2.reg}{weight for l2 regularization, optional.}

\item{early.stop}{Indicate whether early stop is used (only if there exists a validation set).}

\item{early.stop.det}{Number of epochs of increasing loss to determine the early stop.}

\item{learning.rate.adaptive}{Adaptive learning rate adjustment methods, one of the following,
"constant", "adadelta", "adagrad", "momentum", "adam".}

\item{rho}{A parameter used in momentum.}

\item{epsilon}{A parameter used in Adagrad and Adam.}

\item{beta1}{A parameter used in Adam.}

\item{beta2}{A parameter used in Adam.}

\item{loss.f}{Loss function of choice.}
}
\value{
Returns a \code{list} of results to \code{dnnet}.
}
\description{
Back Propagation
}
