#!/usr/bin/env Rscript
library("optparse")

option_list = list(
  make_option(c("-s1", "--seed1"), type="numeric", default=20180516,
              help="seed", metavar="character"),
  make_option(c("-n1", "--sample_size"), type="numeric", default=10000,
              help="seed", metavar="character"),
  make_option(c("-p1", "--n_p"), type="numeric", default=5,
              help="seed", metavar="character"),
  make_option(c("-s2", "--seed2"), type="numeric", default=1111,
              help="seed", metavar="character")
);

opt_parser = OptionParser(option_list=option_list);
opt = parse_args(opt_parser);

# setwd("/home/mi/Dropbox/medical_image/")
setwd("/ufrc/zou/xlmi/medical_image/regression_sim20180708/")
library(keras)
library(MASS)
library(ROCR)
library(kerasR)

# Data Preparation -----------------------------------------------------

generate_resp <- function(x, y0, sigma = 1) {
  
  y <- x[,1]**2 + x[,2]**2 + y0*x[,3] + rnorm(length(y0))*sigma
}

sample_size <- opt$sample_size
# training_size <- 8000
n_p <- opt$n_p
test_size <- 10000
sample_seed <- opt$seed1
test_seed <- opt$seed2

batch_size <- 128
epochs <- 40

# Input image dimensions
img_rows <- 28
img_cols <- 28

# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
set.seed(sample_seed)
ind_sample <- sample(dim(mnist$train$x)[1], sample_size)
x_train <- mnist$train$x[ind_sample,,]
y_train <- mnist$train$y[ind_sample]
x_demo_train <- mvrnorm(sample_size, rep(0, n_p), diag(n_p))
y_train <- generate_resp(x_demo_train, y_train)

set.seed(test_seed)
ind_test <- sample(dim(mnist$test$x)[1], test_size)
x_test <- mnist$test$x[ind_test,,]
y_test <- mnist$test$y[ind_test]
x_demo_test <- mvrnorm(test_size, rep(0, n_p), diag(n_p))
y_test0 <- generate_resp(x_demo_test, y_test, sigma = 0)
y_test <- y_test0 + rnorm(test_size)


# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)

# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255



# # Convert class vectors to binary class matrices
# y_train <- to_categorical(y_train, num_classes)
# y_test <- to_categorical(y_test, num_classes)

# Define Model -----------------------------------------------------------
cat("Model CNN+DNN... \n")
print(Sys.time())

# model <- keras_model_sequential()
img_input <- layer_input(shape = input_shape, name = 'img')

model <- 
  img_input %>% 
  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_conv_2d(filters = 16, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.5) 

# model2 <- keras_model_sequential()
demo_input <- layer_input(shape = n_p, name = 'demo')

model2 <- # demo_input
  demo_input %>% 
  layer_dense(units = 64, activation = 'relu')

model_concat <- 
  layer_concatenate(list(model, model2)) %>%
  # layer_dropout(rate = 0.1) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.1) %>% 
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.1) %>% 
  layer_dense(units = 32, activation = 'relu') %>%
  # layer_dropout(rate = 0.1) %>%
  layer_dense(units = 1, activation = 'linear')

model_keras <- keras_model(
  inputs = c(img_input, demo_input),
  outputs = model_concat
)

# summary(model_keras)

# Compile model
model_keras %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam(),
  metrics = 'mean_squared_error'
)

model_keras %>% fit(
  list(img = x_train, demo = x_demo_train), 
  y_train,
  batch_size = batch_size,
  epochs = epochs,
  # verbose = 0,
  validation_split = 0.2, 
  callbacks = ModelCheckpoint(paste(opt$seed1, opt$seed2, 'double', sep = '.'), save_best_only = TRUE)
)

model_keras <- load_model_hdf5(paste(opt$seed1, opt$seed2, 'double', sep = '.'))
file.remove(paste(opt$seed1, opt$seed2, 'double', sep = '.'))

scores <- model_keras %>% evaluate(
  list(img = x_test, demo = x_demo_test), y_test, verbose = 0
)

y_pred_score <- predict(model_keras, list(img = x_test, demo = x_demo_test))

####################
cat("Model DNN(ALL)... \n")
print(Sys.time())
x_train_concat <- cbind(array_reshape(x_train, c(nrow(x_train), img_rows*img_cols)), x_demo_train)
x_test_concat <- cbind(array_reshape(x_test, c(nrow(x_test), img_rows*img_cols)), x_demo_test)

model <- keras_model_sequential()
model %>% 
  layer_dense(units = 128, activation = 'relu', input_shape = dim(x_train_concat)[2]) %>% 
  # layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 64, activation = 'relu') %>%
  # layer_dropout(rate = 0.1) %>%
  layer_dense(units = 1, activation = 'linear')

# summary(model)

model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam(),
  metrics = c('mean_squared_error')
)

model %>% fit(
  x_train_concat, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 0,
  validation_split = 0.2, 
  callbacks = ModelCheckpoint(paste(opt$seed1, opt$seed2, 'double', sep = '.'), save_best_only = TRUE)
)

model <- load_model_hdf5(paste(opt$seed1, opt$seed2, 'double', sep = '.'))
file.remove(paste(opt$seed1, opt$seed2, 'double', sep = '.'))

scores_concat <- model %>% evaluate(
  x_test_concat, y_test,
  verbose = 0
)

y_pred_score_concat <- predict(model, x_test_concat)

######################
cat("Model DNN0... \n")
print(Sys.time())
model0 <- keras_model_sequential()
model0 %>% 
  layer_dense(units = 128, activation = 'relu', input_shape = n_p) %>% 
  # layer_dropout(rate = 0.3) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 64, activation = 'relu') %>%
  # layer_dropout(rate = 0.1) %>%
  layer_dense(units = 1, activation = 'linear')

# summary(model0)

model0 %>% compile(
  loss = 'binary_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

model0 %>% fit(
  x_demo_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 0,
  validation_split = 0.2, 
  callbacks = ModelCheckpoint(paste(opt$seed1, opt$seed2, 'double', sep = '.'), save_best_only = TRUE)
)

model0 <- load_model_hdf5(paste(opt$seed1, opt$seed2, 'double', sep = '.'))
file.remove(paste(opt$seed1, opt$seed2, 'double', sep = '.'))

scores_demo <- model0 %>% evaluate(
  x_demo_test, y_test,
  verbose = 0
)

y_pred_score_demo <- predict(model0, x_demo_test)



####
# png(paste(opt$sample_size, opt$n_p, opt$seed1, opt$seed2, "pred.png", sep = '.'))
# par(mfrow = c(2, 2))
# plot(y_test, y_pred_score, ylab = "predicted Y", xlab = "Y", main = "CNN + DNN")
# mtext(paste("MSE =", round(mean((as.numeric(y_pred_score) - y_test)**2), 3)), side = 3)
# abline(0, 1)
# plot(y_test, y_pred_score_concat, ylab = "predicted Y", xlab = "Y", main = "DNN (All Info)", col = 3)
# mtext(paste("MSE =", round(mean((as.numeric(y_pred_score_concat) - y_test)**2), 3)), side = 3)
# abline(0, 1)
# plot(y_test, y_pred_score_demo, ylab = "predicted Y", xlab = "Y", main = "DNN (No Image Info)", col = 4)
# mtext(paste("MSE =", round(mean((as.numeric(y_pred_score_demo) - y_test)**2), 3)), side = 3)
# abline(0, 1)
# plot(y_test, y_test0, ylab = "E(Y)", xlab = "Y", main = "DNN", col = 2)
# mtext(paste("MSE =", round(mean((as.numeric(y_test0) - y_test)**2), 3)), side = 3)
# abline(0, 1)
# dev.off()

write.csv(c(mean((as.numeric(y_pred_score) - y_test0)**2), 
            mean((as.numeric(y_pred_score_concat) - y_test0)**2), 
            mean((as.numeric(y_pred_score_demo) - y_test0)**2), 
            mean((as.numeric(y_test) - y_test0)**2), 
            mean((as.numeric(y_pred_score) - y_test)**2), 
            mean((as.numeric(y_pred_score_concat) - y_test)**2), 
            mean((as.numeric(y_pred_score_demo) - y_test)**2)), 
          paste(opt$sample_size, opt$n_p, opt$seed1, opt$seed2, "mse.csv", sep = '.'))

